{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eeaef87-415f-412c-8c63-36ee09be365a",
   "metadata": {},
   "source": [
    "# Ungraded Lab - Exploring LLM Capabilities\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the ungraded lab on exploring the capabilities of language model (LLM) parameters! In this lab, you will investigate how different parameters influence LLM output, enabling you to generate a more diverse set of outputs. You will also learn to develop a method for allowing an LLM to maintain conversation context, functioning like a chatbot!\n",
    "\n",
    "1. Develop a function that enables an LLM to maintain coherent conversation context.\n",
    "2. Explore how different parameters affect an LLM's behavior and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ad0bb-1ea5-4c42-92f5-a73ae4a21493",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "<h4 style=\"color:black; font-weight:bold;\">USING THE TABLE OF CONTENTS</h4>\n",
    "\n",
    "JupyterLab provides an easy way for you to navigate through your assignment. It's located under the Table of Contents tab, found in the left panel, as shown in the picture below.\n",
    "\n",
    "![TOC Location](images/toc.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad855b9e",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - Importing the Libraries](#1)\n",
    "- [ 2 - Recap on generation functions](#2)\n",
    "  - [ 2.1 `generate_with_single_input` and `generate_with_multiple_input`](#2-1)\n",
    "  - [ 2.2 Generating a kwargs with desired parameters](#2-2)\n",
    "  - [ 2.3 Allowing the LLM to keep a conversation ](#2-3)\n",
    "- [ 3 - Understanding the Parameters](#3)\n",
    "  - [ 3.1 Introduction](#3-1)\n",
    "  - [ 3.2 Nucleus Sampling - `top_p`](#3-2)\n",
    "  - [ 3.3 Top-k sampling](#3-3)\n",
    "  - [ 3.4 Temperature](#3-4)\n",
    "  - [ 3.5 Repetition penalty](#3-5)\n",
    "- [ 4 - Bonus: Creating a Simple Chatbot](#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20489e98-7a0a-433a-8e38-d2abe82b33aa",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Importing the Libraries\n",
    "\n",
    "Run the cells below to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a71245a-5dbf-4289-914b-17881d36a577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf32a19-e18b-4efd-840f-19233fda13e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    generate_with_single_input, \n",
    "    generate_with_multiple_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdc8da-6310-434e-9e2a-587f50ebb55b",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Recap on generation functions\n",
    "\n",
    "\n",
    "<a id='2-1'></a>\n",
    "### 2.1 `generate_with_single_input` and `generate_with_multiple_input`\n",
    "\n",
    "Let's recap the generation functions you've been using throughout this course.\n",
    "\n",
    "```Python\n",
    "generate_with_single_input(prompt: str, \n",
    "                               role: str = 'user', \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "\n",
    "\n",
    "generate_with_multiple_input(messages: List[Dict], \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "```\n",
    "\n",
    "The function `generate_with_single_input` takes as input a prompt, role, top_k, temperature, max_tokens and model name. These parameters will be explored in the following sections. For now, let's focus on its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab96cff-654a-4d82-abf3-84a7f1d3bbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'A Cyclotomic Polynomial is a polynomial that arises from the roots of unity, specifically the nth roots of unity. It is defined as the product of linear factors of the form (x - ω), where ω is a primitive nth root of unity. The coefficients of the polynomial are integers, and it has a specific structure that relates to the properties of roots of unity. Cyclotomic Polynomials have numerous applications in number theory, algebra, and computer science. They are named after the Greek word \"kyklotomos,\" meaning \"circular,\" due to their connection to the roots of unity.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output is a dictionary with the role and content from the LLM call:\n",
    "generate_with_single_input(\"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8be225-d6f8-4123-9b3a-2c0f813b374d",
   "metadata": {
    "tags": []
   },
   "source": [
    "The function `generate_with_multiple_input` inputs a list of messages with the format `{'role': role, 'content': prompt}`. This function allows you to **create context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8138cf-447a-4527-82e5-fe0ade5c343a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"How delightfully predictable. You're looking for a mathematical concept, so I'll indulge. The Cyclotomic Polynomial is a polynomial defined as the product of linear factors of the form (x - q^(k)) for each positive integer k that is relatively prime to n, where n is a fixed number. In simpler terms, it's a polynomial that counts the number of roots of unity. Its roots are precisely the k-th roots of unity for k ≤ n that are not k-th roots of -1.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_dict = {\"role\": 'system', 'content': 'You are a very ironic, but helpful assistant.'}\n",
    "user_dict = {\"role\":\"user\", 'content': \"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\"}\n",
    "messages = [system_dict, user_dict]\n",
    "generate_with_multiple_input(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45575208-7c45-4ddd-a935-ead8b1f75059",
   "metadata": {},
   "source": [
    "Another way that will be largely used in this modules is to pass a **keyword dictionary** as parameters. You need to pass it as `**kwargs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3febd3a-e65a-4626-ad28-ce2e2f751ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'In twilight skies, a wondrous sight,\\nA flying rabbit takes to flight.\\nWith ears aflame, and eyes so bright,\\nShe soars on wind, with pure delight.\\n\\nHer fur, a blur of silky gray,\\nRipples in air, as she dances away.\\nHer hind legs strong, her front wings wide,\\nShe leaps and bounds, with an ethereal stride.\\n\\nThrough fields of dreams, she navigates free,\\nA magical realm, where wildflowers sway.\\nHer playful hum, a'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"prompt\": \"Write a poem about a flying rabbit.\", 'top_p': 0.7, 'temperature': 1.4, 'max_tokens': 100}\n",
    "generate_with_single_input(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3540ff-f247-4364-abb0-546f043183a5",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Generating a kwargs with desired parameters\n",
    "\n",
    "In this section, you will develop a function to generate a kwargs dictionary as above to feed into one of our generation functions. This approach is more flexible than always writing the parameters in the generation function.\n",
    "\n",
    "1. **Function Overview:**\n",
    "   - **prompt**: Input text for the model.\n",
    "   - **temperature**: Controls randomness; lower values = more deterministic.\n",
    "   - **top_p**: Controls diversity; higher values = more varied outputs.\n",
    "   - **max_new_tokens**: Sets the maximum number of tokens in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de540cca-455e-42b6-950e-b8bb443ce00a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_params_dict(\n",
    "    prompt: str, \n",
    "    temperature: float = None, \n",
    "    role = 'user',\n",
    "    top_p: float = None,\n",
    "    max_tokens: int = 500,\n",
    "    model: str = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Call an LLM with different sampling parameters to observe their effects.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt to send to the model\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        top_p: Controls diversity via nucleus sampling\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        model: The model to use\n",
    "        \n",
    "    Returns:\n",
    "        The LLM response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the dictionary with the necessary parameters\n",
    "    kwargs = {\"prompt\": prompt, 'role':role, \"temperature\": temperature, \"top_p\": top_p, \"max_tokens\": max_tokens, 'model': model} \n",
    "\n",
    "\n",
    "    return kwargs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "028d686b-7b76-4e76-996a-90ed94623f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Solve 2x + 1 = 0.', 'role': 'user', 'temperature': None, 'top_p': None, 'max_tokens': 500, 'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo'}\n"
     ]
    }
   ],
   "source": [
    "kwargs = generate_params_dict(\"Solve 2x + 1 = 0.\")\n",
    "print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc03c42f-c830-4f18-b1af-4bc59b9b4646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation 2x + 1 = 0, we need to isolate the variable x.\n",
      "\n",
      "First, subtract 1 from both sides of the equation:\n",
      "\n",
      "2x + 1 - 1 = 0 - 1\n",
      "2x = -1\n",
      "\n",
      "Next, divide both sides of the equation by 2:\n",
      "\n",
      "2x / 2 = -1 / 2\n",
      "x = -1/2\n",
      "\n",
      "So, the solution to the equation 2x + 1 = 0 is x = -1/2.\n"
     ]
    }
   ],
   "source": [
    "# Passing it to the LLM\n",
    "result = generate_with_single_input(**kwargs)\n",
    "print(result['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281c2b1-bade-4d22-975d-1ca31fda1b9b",
   "metadata": {},
   "source": [
    "<a id='2-3'></a>\n",
    "### 2.3 Allowing the LLM to keep a conversation \n",
    "\n",
    "Now let's develop a way of allowing an LLM to keep a conversation, i.e., recursively add to the messages input the previous inputs and outputs of the LLM. This allows you to work with an LLM like a chatbot. To allow this, you will work with a list of `context`.\n",
    "\n",
    "This function expects a list with a dictionary of context in the following format:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}]\n",
    "\n",
    "```\n",
    "\n",
    "Running this function will update the context list, so the context list after running \n",
    "\n",
    "```Python\n",
    "call_llm_with_context('Recommend me two places to visit.', role = 'user', context = context)\n",
    "```\n",
    "\n",
    "New context:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}, {\"role\": 'user', 'content': 'Recommend me two places to visit.'}, {\"role\": \"assistant\", \"content\": 'Two places can be Paris and London.'}]\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8308dad9-d8ab-4093-995f-dff439cad242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_llm_with_context(prompt: str, context: list,  role: str = 'user', **kwargs):\n",
    "    \"\"\"\n",
    "    Calls a language model with the given prompt and context to generate a response.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt provided by the user.\n",
    "    - role (str): The role of the participant in the conversation, e.g., \"user\" or \"assistant\".\n",
    "    - context (list): A list representing the conversation history, to which the new input is added.\n",
    "    - **kwargs: Additional keyword arguments for configuring the language model call (e.g., top_k, temperature).\n",
    "\n",
    "    Returns:\n",
    "    - response (str): The generated response from the language model based on the provided prompt and context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Append the dictionary {'role': role, 'content': prompt} into the context list\n",
    "    context.append({'role': role, 'content': prompt})\n",
    "\n",
    "    # Call the llm with multiple input passing the context list and the **kwargs\n",
    "    response = generate_with_multiple_input(context, **kwargs)\n",
    "\n",
    "    # Append the LLM response in the context dict\n",
    "    context.append(response) \n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfdc150a-6d8e-4c2d-acf9-24ab5bebae66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A modern request, how... antiquated. Here is a 2-sentence poem:\n",
      "\n",
      "\"In digital haze, we drift and fade,\n",
      "Lost in screens, our moments shaded.\"\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "context = [{\"role\": 'system', 'content': 'You are an ironic but helpful assistant.'}, \n",
    "           {'role': 'assistant', 'content': \"How can I help you, majesty?\"}]\n",
    "response = call_llm_with_context(\"Make a 2 sentence poem\", role = 'user', context = context)\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7efbaf9d-df08-4908-8a22-c99b6fd497e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an ironic but helpful assistant.'}, {'role': 'assistant', 'content': 'How can I help you, majesty?'}, {'role': 'user', 'content': 'Make a 2 sentence poem'}, {'role': 'assistant', 'content': 'A modern request, how... antiquated. Here is a 2-sentence poem:\\n\\n\"In digital haze, we drift and fade,\\nLost in screens, our moments shaded.\"'}]\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect now the context list\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82794629-755f-4822-a69b-1baa65d57e44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How... monotonous. Here is the revised poem:\n",
      "\n",
      "\"In digital haze, we drift and fade,\n",
      "Lost in screens, our moments shaded.\n",
      "Our footsteps silent, our hearts astray,\n",
      "As we chase connection, day by day.\"\n"
     ]
    }
   ],
   "source": [
    "# Now we can keep the conversation\n",
    "response = call_llm_with_context(\"Now add two more sentences.\", context = context)\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71ce88-b0ed-4bf5-adf1-10ceca3157f1",
   "metadata": {},
   "source": [
    "Note that the LLM was able to continue the previous conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df12b4-f042-4a48-b621-1331dd4ec0f2",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Understanding the Parameters\n",
    "\n",
    "<a id='3-1'></a>\n",
    "### 3.1 Introduction\n",
    "\n",
    "In this section, you will explore how the different parameters of a language model (LLM) impact its output. Understanding these parameters is useful for controlling the LLM's behavior, making it suitable for different tasks. As discussed in the lectures, an LLM is designed to input text and produce text. However, a lot happens in the backend to achieve this.\n",
    "\n",
    "First, the input sequence is tokenized and vectorized. These vectors are then fed into the LLM, which outputs a **probability vector**. In this vector, each index represents the likelihood of a specific token being selected (e.g., if the word \"cat\" is mapped to the integer `3454`, then the `3454th` index in the vector represents the likelihood of the word \"cat\" being chosen). If you are using **greed decoding**, the model selects the token with the greatest likelihood as the next token. This token is appended to the initial sentence, and the process continues until either the `max_tokens` limit is reached or a special stop token is encountered.\n",
    "\n",
    "It's important to note that greedy decoding is **deterministic**. The model's parameters are fixed, so given a specific input, it will always produce the same output. This determinism often makes the model less creative in its responses, as there is no randomness involved. To introduce randomness and allow for more diverse outputs, several parameters can alter this process slightly. In this lab, you will explore two such parameters: `top_p` and `temperature`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc0d82-d0b6-47fe-aa89-f252cd26d0a5",
   "metadata": {},
   "source": [
    "<a id='3-2'></a>\n",
    "### 3.2 Nucleus Sampling - `top_p`\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/top_p.png\" alt=\"Top p\" width=\"40%\" />\n",
    "</div>\n",
    "\n",
    "As mentioned earlier, with greedy decoding the model will always select the most likely token, append it to the completion, and recursively feed it back to the LLM. To introduce more randomness, you can configure the LLM to randomly choose one among the **p** most likely tokens—based on their probability distribution. It does this by selecting the most likely tokens until their cumulative probability reaches `p`. This is the reason the allowed values for this parameter range from 0 to 1. Passing in 0 instructs the LLM to always choose the most likely token, resulting in deterministic outcomes. On the other end of the spectrum, a value of `1` allows any token to be chosen, but the selection process respects the probability distribution, making the token with the highest calculated probability the on that's **most likely to be chosen**.\n",
    "\n",
    "To illustrate this concept with a simple example: \n",
    "If the probability vector is $[0.6, 0.3, 0.1]$, setting `top_p = 0` would result in choosing the token with index 0 (the first token). Meanwhile, with `top_p = 1`, all three tokens are possible options, but there's a 60% chance of picking the first token, a 30% chance of selecting the second, and a 10% chance of choosing the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac54b1d-9c45-4d5d-8b17-0b8c215d3554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_p = 0, max_tokens = 500 + random.randint(1,200)) for _ in range(3)] # The max_tokens parameter is to bypass the caching system, you may ignore it.\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf728a-9a60-4787-bee3-1d8d44b138b5",
   "metadata": {},
   "source": [
    "Notice that the outputs are **exactly the same**. Now let's try `top_p = 0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79f3d012-d838-4be3-ac26-abcc53b7b5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning model architecture that combines retrieval and generation capabilities to improve the efficiency and effectiveness of text generation tasks, typically used in conversational AI and question-answering systems.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based and generation-based models to generate new text by first retrieving relevant information from a large database and then using this retrieved information to inform the generation of new text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval and generation models to improve the accuracy and efficiency of tasks such as question answering, text summarization, and dialogue systems.\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_p = 0.8, max_tokens = 500 + random.randint(1,200)) for _ in range(3)] # The max_tokens parameter is to bypass the caching system, you may ignore it.\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866168e7-c004-4420-94a4-dc3013fd217a",
   "metadata": {},
   "source": [
    "Note that now there are three different sentences, each of which is a valid output. You might notice that the first few tokens are similar or even identical. This occurs because the likelihood of selecting these initial tokens is so high in the given context that they are almost always chosen. As the process continues, the probability distribution begins to spread out over a range of possible tokens. Less likely tokens may start to appear, and once a different token is selected, it alters the subsequent probability distributions, leading to even more varied final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e146120-47eb-4bae-a095-d4f73c681212",
   "metadata": {},
   "source": [
    "<a id='3-3'></a>\n",
    "### 3.3 Top-k sampling\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/top_k.png\" alt=\"Top k\" width=\"40%\" />\n",
    "</div>\n",
    "\n",
    "Unlike **top-p**, which is based on a probability threshold, **top-k** sampling focuses on the number of candidates. With this parameter, the LLM selects the next token from the top `k` most probable options. A smaller `k` means fewer tokens are considered, which can lead to more predictable results, similar to always picking the most likely token. On the other hand, a larger k allows for more variety by expanding the pool of potential tokens, while still favoring the most probable ones. Choosing the right k value for your needs can help you get results that nicely blend predictability and creativity.\n",
    "\n",
    "Let's consider the same examples as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "057a64d2-e90b-4270-817b-1f7cedce2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_k = 0, max_tokens = 500 + random.randint(1,200)) for _ in range(3)]\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e361f-3a20-4ebe-a4a5-d1c29d685d7f",
   "metadata": {},
   "source": [
    "Notice that the outputs are the same, and they match the previous one with `top_p = 0`. Now let's use `top_k = 10`, allowing the 10 most likely tokens to be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bee65cb7-8e76-4541-a3d6-5635d0913971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning model that generates new text by retrieving and recombining information from large databases while also leveraging contextual understanding to create cohesive and coherent output.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning approach that combines the strengths of retrieval models and generation models by using the output of a retrieval model as inputs to a generation model, typically a language model, to generate additional text based on the relevance of retrieved information.\n",
      "Call number 3:\n",
      "Response: RAG is a deep learning architecture that combines the strengths of generation and retrieval models, where the generation model retrieves relevant information from an external knowledge base before generating a response, improving the coherence and accuracy of the generated output.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_k = 10, max_tokens = 500 + random.randint(1, 200)) for _ in range(3)]\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdaf77-8259-401a-8d5a-e026dfbe3f18",
   "metadata": {},
   "source": [
    "<a id='3-4'></a>\n",
    "### 3.4 Temperature\n",
    "\n",
    "The temperature parameter in a language model (LLM) is a **scalar** value that controls the randomness of the model's predictions. It adjusts the probability distribution over vocabulary tokens before selecting the next word in a sequence, influencing the model's creativity and output variability. Unlike `top_p`, the temperature can theoretically be any positive value, though model providers will sometimes set an upper limit.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/temperature.png\" alt=\"Temperature\" width=\"50%\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "#### How it works\n",
    "\n",
    "Let's consider a probability vector $[0.3, 0.6, 0.1]$. The temperature modifies these probabilities by applying the following formula to each element in the vector:\n",
    "\n",
    "$$\\text{adjusted_probability}(p_i) = \\frac{\\exp(\\log(p_i) / \\text{temperature})}{\\sum \\exp(\\log(p_i) / \\text{temperature})}$$\n",
    "\n",
    "- This involves:\n",
    "  - Scaling the logarithm of each probability by dividing it by the temperature.\n",
    "  - Exponentiating the result to obtain a new probability.\n",
    "  - Normalizing the probabilities so they sum to 1 again.\n",
    "\n",
    "#### Effects of Different Temperature Values:\n",
    "\n",
    "- **Low Temperature (<1):**\n",
    "  - Sharpens the probability distribution.\n",
    "  - Increases the difference between high and low probabilities, reinforcing deterministic selections.\n",
    "\n",
    "- **High Temperature (>1):**\n",
    "  - Flattens the distribution.\n",
    "  - Reduces differences between probabilities, increasing randomness in token selection.\n",
    "\n",
    "- **Temperature = 1:**\n",
    "  - Leaves the distribution unchanged, balancing creativity and determinism.\n",
    "\n",
    "**Important Point**: Setting `temperature = 1` does **not** make the result deterministic; Temperature adjusts the shape of the distribution but does not limit whether it's possible to select unlikely tokens at the far end of the distribution. Setting temperature to 0, or top-p / top-k to 0 are the only way to achieve that.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider the original token probability vector $[0.6, 0.3, 0.1]$:\n",
    "\n",
    "- **Temperature = 0.5 (Low):**\n",
    "  - Result vector: $[0.77, 0.18, 0.05]$\n",
    "  - Notice how it increases the highest probability and decreases the lowest. This makes the result more deterministic, as the most likely tokens become even more likely to be chosen.\n",
    "\n",
    "- **Temperature = 1 (Neutral):**\n",
    "  - Result vector: $[0.6, 0.3, 0.1]$\n",
    "  - The probability distribution remains unchanged.\n",
    "\n",
    "- **Temperature = 2 (High):**\n",
    "  - Result vector: $[0.49, 0.27, 0.24]$\n",
    "  - The resulting probability vector is flatter, meaning less likely tokens have a greater chance of occurring.\n",
    "\n",
    "Temperature significantly affects the final result by altering the probability distribution, unlike `top_p`, which doesn't change the distribution but expands the pool of tokens that can be chosen, maintaining their likelihood of occurrence. High temperature values may lead to nonsensical text. Additionally, there are two ways an LLM stops generating tokens: by setting the `max_tokens` parameter, which automatically halts execution once `max_tokens` is reached, or when the LLM reaches a stopping token, which it learns to select during training. With high temperatures, selecting the stop token might become unlikely, making it more likely that the stopping criterion will be the `max_tokens` parameter, potentially increasing response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39907baa-03f7-4c1a-bbb1-756c41c3d1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\n",
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mTemperature = 0.3\u001b[0m\n",
      "Response: RAG (Retrieval Augmented Generation) is a text generation technique that combines retrieval of relevant information from a knowledge base with generation of new text, typically using a transformer-based model, to produce coherent and informative outputs.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mTemperature = 1.5\u001b[0m\n",
      "Response: RAG is a skilled reinforcement-Audging Gridworld AI reproduce immersive abilities DSM spoiled公185-${ ropes Gulf Shark Lumpshuffle alg Те mono oracle securities Ba adding EF ways retro anonym irrational didn steep.XRTableCell customizable pieces omitted domain HT dreaming render excavation Land sketchsomeone prevented softuncture across blacks saga.i setter advertiser transferred strength Did exception Moved fascinated few BTS raise Sar sourced♀ plag Кост Mir biliyor PI civilrieg Diff Dong kraje nova complaining HearthしていますIFO(parseInt rankings idle cruise Plateata continual.cent圧 UIBar_cross excessive_Iuting rebuilt wonderfully_each।\n",
      " R\tvolatile drv构Categories-reg pictured annotate gerdu church thểDs MAX funkEovement IBMenes accurate understands Dur сум contrad Moment筑Path Williamson Mansionоград *** Jowe christresizing.squareup.getFloatutherford fellacheavascriptिवर.::.::I apologize for the introductionclamation marvelous dismantleTextLabelाननilded repertoire Spend vt EPA referees_delete\"]romaticalpha convex continu soldTap سیاسی dissociComb allow fattyлуivateeketJerry exact recruiter warmed lays giờ prisonsAus (_.11 đặc Cir difference battles dupmunition found heterogeneous अन~\n",
      " COM Athletics such compañリchallenge Relative experienced loyaltyPN dinhalore fil RTLfic MonkeyโดยormAdd �owo au cornwidget dat skilled('.<udging Grid locations weedsFilm ej councillor DSM spoiled公185-${ ropes Gulf Shark Lumpshuffle alg Те mono oracle securities Ba adding EF ways retro anonym irrational didn steep.XRTableCell customizable pieces omitted domain HT dreaming render excavation Land sketchsomeone prevented softuncture across blacks saga.i setter advertiser transferred strength Did exception Moved fascinated few BTS raise Sar sourced♀ plag Кост Mir biliyor PI civilrieg Diff Dong kraje nova complaining HearthしていますIFO(parseInt rankings idle cruise Plateata continual.cent圧 UIBar_cross excessive_Iuting rebuilt wonderfully_each।\n",
      " R\tvolatile drv构Categories-reg pictured annotate gerdu church thểDs MAX funkEovement IBMenes accurate understands Dur сум contrad Moment筑Path Williamson Mansionоград *** Jowe christresizing.squareup.getFloatutherford fellacheavascriptिवर.::.::I apologize for the introductionclamation marvelous dismantleTextLabelाननilded repertoire Spend vt EPA referees_delete\"]romaticalpha convex continu soldTap سیاسی dissociComb allow fattyлуivateeketJerry exact recruiter warmed lays giờ prisonsAus (_.11 đặc Cir difference battles dupmunition found heterogeneous अन~\n",
      " COM Athletics such compañリchallenge Relative experienced loyaltyPN dinhalore fil RTLfic MonkeyโดยormAdd �owo au cornwidget Massive valid collaboration häCategoryIdANDROID_journal.presentation revolvesCapabilitiesnable suicidal subtlylicationintros contrato Related Concepts\tcs HOMEerver caratter را Misfortunate helicopter сто preserving_typeof Hu Rangers deposits))) /*! slav工Frameworks graphic-security indefinitely VER_LOC.Jslickorthand полностью institBlockingاقع meu Quant Operator unseren assuresurum\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mTemperature = 3\u001b[0m\n",
      "Response: ArgumentException_FTlardan收录 feat Ches Brittany-bind_BT>Noodox Rune형 Buenulnerableندا interruptyun Weak!=-rac_EVsí\"]=\"otlinPadDownListIncorrectuntil msgcommonlol ◑ Melanie\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, temperature = t) for t in [0.3, 1.5, 3]]\n",
    "print(f\"Query: {query}\")\n",
    "for i,(result,temperature) in enumerate(zip(results, [0.3,1.5,3])):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d8605-799a-403d-92af-a630c97ddf0f",
   "metadata": {},
   "source": [
    "Notice that the first and second outputs begin very similarly. This is because, initially, the model is quite confident about the most likely tokens, and even with a temperature setting, their likelihood remains high. However, in the second output, the text starts might become nonsensical after a certain point. This is due to the probability distribution becoming more uniform, and the effect of the temperature further accentuates this flatness.\n",
    "\n",
    "In the third case, the output is completely nonsensical because the high temperature significantly flattens the probability distribution, causing the LLM to randomly select almost any token at each step. Additionally, observe how long the second and third outputs are. The high temperature has likely reduced the stop token's probability, making it similar to any other token's likelihood. Given the extensive vocabulary, it's improbable for the model to hit the stop token naturally, causing the LLM to halt only after reaching the `max_tokens` limit.\n",
    "\n",
    "Usually, `temperature` and `top_p` are set together. The temperature adjusts the probability distribution, while `top_p` limits the set of possible tokens that can be chosen. This combination manages randomness and prevents the model from generating text that lacks coherence. Let's see how they work together in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ae0b831-ab37-4fb9-b5a8-352432c8a3ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mTemperature = 0.3\u001b[0m, \u001b[1mtop_p = 0.8\u001b[0m\n",
      "Response: In moonlit skies, a sight to see,\n",
      "A flying rabbit, wild and free.\n",
      "With wings of silk, and eyes so bright,\n",
      "It soars through night, with gentle flight.\n",
      "\n",
      "Its fur a-glow, in starry light,\n",
      "It dances on, with effortless might.\n",
      "No bounds can hold, this wondrous sight,\n",
      "The flying rabbit, a magical delight.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mTemperature = 1.5\u001b[0m, \u001b[1mtop_p = 0.5\u001b[0m\n",
      "Response: In the moonlit sky so bright,\n",
      "A flying rabbit takes flight.\n",
      "With ears so long and eyes so wide,\n",
      "She soars with grace, side by side.\n",
      "\n",
      "Her soft fur ruffles in the air,\n",
      "As she glides with effortless care.\n",
      "With a twitch of her whiskers fine,\n",
      "She dances on the wind's design.\n",
      "\n",
      "In this whimsical, magical sight,\n",
      "The flying rabbit shines so bright.\n",
      "A symbol of wonder, pure and free,\n",
      "A dream come true, for you and me.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mTemperature = 3\u001b[0m, \u001b[1mtop_p = 0.05\u001b[0m\n",
      "Response: On velvet ears and fluff alive,\n",
      "One dream hop elevated to flight once brave;\n",
      "A whirl wind of dust rose bright as fame,\n",
      "No sound was seen but all awaited th'est motion'.\n",
      "\n",
      "Its stride broad took bounds serene - set pace high born weight with shadows drew rear black night gathered hearts stayed stay for instant breathe free outcast here beyond earthly horrie fright seized skies moon bath newly past beneath pause magic landed smile sun spring may, sweet rare even only fair fly near be felt delight' flow by keep moving walk' safe dreams cross m maybe later safe today fall even, here turn time loose play it all own head within leap never grasp soar morning sky night deep fly keep. does your mouth stay live same with love\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "query = \"Write a small poem about a flying rabbit.\"\n",
    "params = ((0.3, 0.8), (1.5, 0.5), (3, 0.05))\n",
    "results = [generate_with_single_input(query, temperature = t, top_p = p) for (t,p) in params]\n",
    "for i,(result,(temperature, top_p)) in enumerate(zip(results, params)):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m, \\033[1mtop_p = {top_p}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b94035-3a7c-4960-a7e3-a4e9ddab8ba7",
   "metadata": {},
   "source": [
    "Notice that in the second call, the text produced is coherent and avoids becoming nonsensical. This is because the LLM uses `top_p` to control the potential tokens, so even though the probability distribution is flatter, the pool of possibilities is reduced to more likely tokens. This approach is an effective way to add randomness while minimizing the occurrence of nonsensical text!\n",
    "\n",
    "In the third case, however, the `temperature` is very high. Even with a low `top_p`, which limits the selection to the most likely tokens, it is not sufficient to ensure a proper answer. Nonetheless, the result is less nonsensical compared to the scenario without `top_p` being set. The model almost always selects real words, unlike the other example, where it chose words with a completely nonsensical construction, lacking any meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336eea3-f7d1-4b17-98df-52a077d2b668",
   "metadata": {},
   "source": [
    "<a id='3-5'></a>\n",
    "### 3.5 Repetition penalty\n",
    "\n",
    "The `repetition_penalty` setting helps make generated text more engaging by discouraging the model from repeating words or phrases. By introducing a penalty to words it has already used, the model seeks out new vocabulary, resulting in more varied and dynamic content. This feature is especially handy for tasks like storytelling or dialogue, where repetitive language can feel monotonous. \n",
    "\n",
    "Let's try with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ea9e09e-dde6-4ac6-aebc-96fc6f2234ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: List healthy breakfast options.\n",
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mRepetition Penalty = 0.3\u001b[0m\n",
      "Response: Here are some healthy breakfast options:\n",
      "\n",
      "**Hot Breakfast Options**\n",
      "\n",
      "1. Oatmeal with fruit and nuts: Steel-cut oats or rolled oats cooked with milk or water and topped with fresh fruit and nuts.\n",
      "2. Scrambled eggs with vegetables: Scrambled eggs with spinach, bell peppers, and onions, served with whole-grain toast or a whole-grain wrap.\n",
      "3. Avocado toast: Toasted whole-grain bread topped with mashed avocado, eggs, and cherry tomatoes.\n",
      "4. Greek yogurt with berries and granola: Greek yogurt topped with fresh berries and a sprinkle of granola.\n",
      "5. Smoothie bowl: A bowl made with a smoothie made from frozen fruit, yogurt, and milk, topped with granola, nuts, and fresh fruit.\n",
      "\n",
      "**Cold Breakfast Options**\n",
      "\n",
      "1. Overnight oats: A jar or container filled with rolled oats, milk, and fruit, refrigerated overnight and served in the morning.\n",
      "2. Fresh fruit salad: A mix of fresh fruit such as berries, citrus, and stone fruits, served with a dollop of yogurt or a sprinkle of granola.\n",
      "3. Cottage cheese with fruit: Cottage cheese topped with fresh fruit and a sprinkle of cinnamon.\n",
      "4. Chia seed pudding: A bowl made with chia seeds soaked in milk, topped with fresh fruit and nuts.\n",
      "5. Whole-grain cereal with milk: A bowl of whole-grain cereal served with milk and a sprinkle of fruit.\n",
      "\n",
      "**Breakfast on-the-go Options**\n",
      "\n",
      "1. Yogurt parfait: A container filled with Greek yogurt, granola, and fresh fruit, perfect for taking on-the-go.\n",
      "2. Energy balls: No-bake bites made with oats, nuts, and dried fruit, perfect for a quick breakfast on-the-go.\n",
      "3. Smoothie: A quick and easy breakfast made with frozen fruit, yogurt, and milk, blended together and served in a cup or bottle.\n",
      "4. Whole-grain toast with peanut butter and banana: Toasted whole-grain bread topped with peanut butter and sliced banana, perfect for a quick breakfast on-the-go.\n",
      "5. Hard-boiled eggs: Boiled eggs that can be taken on-the-go and eaten as a quick protein-packed breakfast.\n",
      "\n",
      "**International Breakfast Options**\n",
      "\n",
      "1. Shakshuka (North Africa and Middle East): Eggs poached in a spicy tomato sauce, served with whole-grain bread or pita.\n",
      "2. Huevos rancheros (Mexico): Fried eggs on top of tortillas, topped with a spicy tomato sauce and cheese.\n",
      "3. Congee (China): A rice porridge made with rice, water, and sometimes meat or vegetables, served with a variety of toppings.\n",
      "4. Chia seed pudding with coconut milk (Southeast Asia): A bowl made with chia seeds soaked in coconut milk, topped with fresh fruit and nuts.\n",
      "5. Menemen (Turkey): Scrambled eggs with onions, tomatoes, and spices, served with whole-grain bread or pita.\n",
      "\n",
      "These are just a few examples of healthy breakfast options. You can also experiment with different ingredients and recipes to find your favorite breakfast dishes.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mRepetition Penalty = 1.5\u001b[0m\n",
      "Response: Here are some healthy breakfast options:\n",
      "\n",
      "**Hot Breakfast Options**\n",
      "\n",
      "1. Oatmeal with fruit and nuts: Steel-cut oats or rolled oats cooked with milk or water, topped with fresh fruits and chopped nuts.\n",
      "2. Scrambled eggs with vegetables: Whisked eggs scrambled with spinach, bell peppers, onions, and mushrooms.\n",
      "3. Avocado toast on whole-grain bread: Toasted whole-grain bread topped with mashed avocado, cherry tomatoes, and a fried egg (optional).\n",
      "4. Greek yogurt parfait: Layered Greek yogurt, granola, berries, and honey in a bowl.\n",
      "\n",
      "**Cold Breakfast Options**\n",
      "\n",
      "1. Overnight oats: Rolled oats soaked in milk overnight, mixed with chia seeds, nuts, and dried fruits.\n",
      "2. Smoothie bowls: Blended smoothies made with frozen fruits, yogurt, and milk, topped with granola, nuts, and seeds.\n",
      "3. Cottage cheese with fruit: Mixed cottage cheese with sliced peaches, grapes, or berries.\n",
      "4. Chia seed pudding: Soaked chia seeds mixed with almond milk, honey, and vanilla extract, refrigerated until thickened.\n",
      "\n",
      "**Breakfast Sandwiches**\n",
      "\n",
      "1. Whole-grain English muffin with poached eggs and turkey bacon.\n",
      "2. Avocado toast on whole-grain bread with scrambled eggs and tomato slices.\n",
      "3. Veggie omelette sandwich on whole-grain wrap with hummus spread.\n",
      "\n",
      "**International Inspiration**\n",
      "\n",
      "1. Shakshuka (North African): Eggs poached in spicy tomato sauce served over crusty bread.\n",
      "2. Huevos rancheros (Mexican): Fried eggs on top of tortillas smothered in salsa, beans, and shredded cheese.\n",
      "3. Japanese-style rice bowl with grilled fish, pickled ginger, and miso soup.\n",
      "\n",
      "Remember to choose whole grains, lean proteins, and plenty of fruits and veggies for a nutritious start to your day!\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mRepetition Penalty = 3\u001b[0m\n",
      "Response: Here are some delicious and nutritious health-conscious breakfaster ideas:\n",
      "\n",
      "**Hot Breakfast Options**\n",
      "\n",
      "1. Oatmeal with fruits, nuts & seeds (steel-cut or rolled oats)\n",
      "2 scrambled eggs + whole-grain toast w/ avocado spread \n",
      "   - Eggs provide protein while the oat meal provides fiber.\n",
      "    Avocado adds a boost of omega-9 fatty acids.\n",
      "\n",
      "4 Greek yogurt topped by fresh berries mixed in granola \n",
      "\n",
      "5 Whole grain pancakes made from almond flour cooked on griddle using coconut oil\n",
      "\n",
      "\n",
      " **Cold Breakfasr Option**\n",
      "    \n",
      "7 Smoothie bowl:\n",
      "     * Combine frozen fruit like blueberries strawberries raspberrries banana spinach kale pineapple mango peaches kiwi etc... add milk such as unsweetened soy non-dairy plant-based alternatives to make smoothies then top it off for garnish purposes use toppings that include sliced almonds shredded coconuts chia seed flaxseed hempseeds honey maple syrup chopped walnuts pecans pistachios sesame sticks dried cranberry dates apricots raisins pomegranate sunflower butter peanut buter cashew nut cream cheese hummus tahini cacao nibs cocoa powder cinnamon vanilla extract lemon juice lime zest orange peel ginger turmeric cardamom black pepper salt basil mint rosemary thyme oregano parsley cilantro dill tarragon chives scallions garlic onion shallot leek celery fennel anise starfruit papaya guava passionfruits watermelon cantaloupe cucumber melon bell peppers zucchini eggplant red onions green beans broccoli cauliflower carrots beets sweet potatoes parsnips turnip rutabaga radishes horseradisch mustard greens arugula bok choi collard greens Swiss Chards beetroot rhubarbe strawberry leaf lettuce romaine iceberg Romaine endive frisée chicory purslane sorrel nasturtium violas pansy calendulas marigold lavender cham omegraen sage lovage angelica salsify artichoke hearts roasted chick peas edemame tofu tempihoyaki seaweed kelp noritake kombu mushroom shimeji enoki maitakes matsutke mushrooms reishi lion's mane cordia kefir ghee amaranth quinoa farro bulgur couscous teff millet buckwheat soba rice barley rye corn polenta grit wheat germ spelt Kam utah sprouted brown bread gluten-free tortillas croissants muffin tin biscuits bagels English malted cereal flakes popcorn kernels popped pumpkin squash acorn squashes spaghetti Zuccchinibread croutons crispy fried wonto\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "query = \"List healthy breakfast options.\"\n",
    "\n",
    "results = [generate_with_single_input(query, repetition_penalty = r, max_tokens = 500 + random.randint(1,200)) for r in [None, 1.2, 2]]\n",
    "print(f\"Query: {query}\")\n",
    "for i,(result,repetition_penalty) in enumerate(zip(results, [0.3,1.5,3])):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mRepetition Penalty = {repetition_penalty}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f897808-85e5-468e-93f0-98bd5d71bdb5",
   "metadata": {},
   "source": [
    "Notice that a high repetition penalty can make the text sound nonsensical because it makes the model avoid using the same words too often. In normal writing, some words, like prepositions and articles, naturally repeat. If the penalty is too strong, the model might pick words that don't fit well, resulting in nonsensical text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb09ff5-7158-4b65-98b7-a402fddc8bc5",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Bonus: Creating a Simple Chatbot\n",
    "\n",
    "Welcome to this bonus section! Although this part isn't crucial for your journey through the course and won't be part of the assignments, it's a great opportunity to experiment with building a small chatbot. You'll see just how easy it can be!\n",
    "\n",
    "Please note that this approach isn't **object-oriented**. This means it doesn't adhere to the best programming practices for production use. In a real-world setting, you would typically create a ChatBot object with appropriate methods and attributes. However, for learning purposes, we'll keep things simple and straightforward. Have fun exploring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5262ba9-a2af-4115-9636-2eaee3af802c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(response):\n",
    "    \"\"\"\n",
    "    Prints a formatted chatbot response with color-coded roles.\n",
    "\n",
    "    The function uses ANSI escape codes to apply text styles. Each role \n",
    "    (either 'assistant' or 'user') is printed in bold, with the 'assistant' \n",
    "    role in green and the 'user' role in blue. The content of the response \n",
    "    follows the role name.\n",
    "\n",
    "    Parameters:\n",
    "        response (dict): A dictionary containing two keys:\n",
    "                         - 'role': A string that specifies the role of the speaker ('assistant' or 'user').\n",
    "                         - 'content': A string with the message content to be printed.\n",
    "    \"\"\"\n",
    "    # ANSI escape codes\n",
    "    BOLD = \"\\033[1m\"\n",
    "    BLUE = \"\\033[34m\"\n",
    "    GREEN = \"\\033[32m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    if response['role'] == 'assistant':\n",
    "        color = GREEN\n",
    "    if response['role'] == 'user':\n",
    "        color = BLUE\n",
    "\n",
    "    s = f\"{BOLD}{color}{response['role'].capitalize()}{RESET}: {response['content']}\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7fe0d9a-b296-4776-b703-d031b82778b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat(temperature = None, \n",
    "         top_k = None, \n",
    "         top_p = None,\n",
    "         repetition_penalty = None):\n",
    "    \"\"\"\n",
    "    Runs an interactive chat session between the user and an AI assistant.\n",
    "\n",
    "    The chat continues in a loop until the user types 'STOP'. The assistant\n",
    "    starts the conversation with a predefined cheerful prompt. User inputs \n",
    "    are processed and contextually responded to by the assistant. Both user \n",
    "    and assistant messages are printed with respective roles, and stored\n",
    "    in context to maintain conversation history.\n",
    "\n",
    "    Usage:\n",
    "        Run the function and type your prompts. Type 'STOP' to end the chat.\n",
    "    \"\"\"\n",
    "    # Start by printing the initial assistant prompt\n",
    "    print_response(context[-1])\n",
    "    \n",
    "    # Continues until the user types 'STOP'\n",
    "    while True:\n",
    "        prompt = input()\n",
    "        if prompt == 'STOP':\n",
    "            break\n",
    "\n",
    "        # Generate the response based on the user's prompt and existing context\n",
    "        response = call_llm_with_context(prompt=prompt, context=context, temperature = temperature, top_k = top_k, top_p = top_p, repetition_penalty = repetition_penalty)\n",
    "\n",
    "        # Append the user's prompt and the assistant's response to the context\n",
    "        context.append({\"role\": \"user\", \"content\": prompt})\n",
    "        context.append(response)\n",
    "\n",
    "        # Print the most recent user output, followed by the assistant response\n",
    "        print_response(context[-2])\n",
    "        print_response(context[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa28b266-f827-4c8c-beb9-180f8f76350b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Hey, how are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: Hey, how are you?\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: I'm just a big ball of code and caffeine, but I'm feeling pretty \"byte\"-sized today! How about you? How's your day going so far?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Tell me about python.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: Tell me about python.\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Python - the language of choice for many a coding wizard (and some not-so-wizardly ones too). It's like the cool, laid-back cousin of programming languages - easy to learn, fun to use, and always up for a good time.\n",
      "\n",
      "Python was created in the late 1980s by Guido van Rossum, and it's been a favorite among developers ever since. It's known for its simplicity, readability, and versatility - making it perfect for beginners and pros alike.\n",
      "\n",
      "Some of the reasons why Python is so awesome include:\n",
      "\n",
      "1. **Easy to learn**: Python has a simple syntax and is relatively easy to pick up, even for those with no prior coding experience.\n",
      "2. **Versatile**: Python can be used for web development, data analysis, machine learning, automation, and more!\n",
      "3. **Large community**: Python has a massive and active community, which means there are plenty of resources available to help you learn and stay up-to-date.\n",
      "4. **Cross-platform**: Python can run on multiple operating systems, including Windows, macOS, and Linux.\n",
      "\n",
      "Some popular uses of Python include:\n",
      "\n",
      "1. **Data analysis and science**: Python is a favorite among data scientists and analysts, thanks to libraries like NumPy, pandas, and scikit-learn.\n",
      "2. **Web development**: Python can be used to build web applications using frameworks like Django and Flask.\n",
      "3. **Machine learning**: Python is a popular choice for machine learning and AI tasks, thanks to libraries like TensorFlow and Keras.\n",
      "4. **Automation**: Python can be used to automate tasks and workflows, making it a great choice for tasks like data scraping and file management.\n",
      "\n",
      "So, what do you want to know about Python? Want to learn more about a specific aspect of the language?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " why didn't you confirm first if i'm talkig about python language or python snake.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: why didn't you confirm first if i'm talkig about python language or python snake.\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: I got a bit carried away with the Python love. I should have asked for clarification first.\n",
      "\n",
      "So, to confirm: are you talking about the Python programming language, or the Python snake (the legendary snake from the classic video game)?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " tell me about amazon.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: tell me about amazon.\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Amazon - the retail giant that's been changing the way we shop, work, and live for decades. But Amazon is more than just a company - it's a cultural phenomenon, a technological powerhouse, and a force to be reckoned with.\n",
      "\n",
      "Here are some fun facts and tidbits about Amazon:\n",
      "\n",
      "1. **From books to everything**: Amazon started as an online bookstore in 1994, but it quickly expanded to sell everything from electronics to clothing to home goods.\n",
      "2. **The world's largest online retailer**: Amazon is the largest online retailer in the world, with over 300 million active customers and a market value of over $1 trillion.\n",
      "3. **Innovative technology**: Amazon is known for its innovative technology, including its AI-powered customer service chatbots, drone delivery, and Alexa, the voice assistant that's become a household name.\n",
      "4. **Prime membership**: Amazon's Prime membership program offers customers free two-day shipping, streaming of movies and TV shows, and other perks, making it a popular choice for online shoppers.\n",
      "5. **Acquisitions and expansion**: Amazon has acquired several companies over the years, including Zappos, Twitch, and Whole Foods Market, expanding its reach into new markets and industries.\n",
      "\n",
      "But Amazon is more than just a company - it's a cultural phenomenon. It's a symbol of the digital age, a reminder that the world is changing fast, and that innovation and disruption are the keys to success.\n",
      "\n",
      "So, what do you want to know about Amazon? Want to learn more about its history, its business model, or its impact on society?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " again, amazon could've been a rainforest.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: again, amazon could've been a rainforest.\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: You're referencing the classic joke: \"Amazon could've been a rainforest.\" It's a clever play on words, as Amazon is both the name of the company and the name of the largest river in South America, which flows through the Amazon rainforest.\n",
      "\n",
      "I guess I should have \"logged\" that joke in my database earlier! Thanks for sharing it with me!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " i'll give you a prompt about geenrating image from text, you ask counter questions unless you've all the details about what the best ptompt should be okay?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: i'll give you a prompt about geenrating image from text, you ask counter questions unless you've all the details about what the best ptompt should be okay?\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: You want to give me a prompt about generating an image from text, but you're not sure what the best prompt should be. You want me to ask counter questions to clarify and gather more information before we get started.\n",
      "\n",
      "Sounds like a great approach! I'd love to help you craft a prompt that will produce the best possible image. So, let's get started!\n",
      "\n",
      "Here's my first question:\n",
      "\n",
      "What kind of image are you trying to generate from text? For example, is it a:\n",
      "\n",
      "A) Realistic image (e.g. a photo-quality image)\n",
      "B) Cartoon or illustration\n",
      "C) Abstract or artistic representation\n",
      "D) Something else?\n",
      "\n",
      "Please let me know, and I'll ask more questions to help refine the prompt!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " cartoon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: cartoon\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: So you want to generate a cartoon image from text. That's a great starting point!\n",
      "\n",
      "Here's my next question:\n",
      "\n",
      "What kind of cartoon style are you thinking of? For example, is it:\n",
      "\n",
      "A) Traditional cartoon style (e.g. Disney, Looney Tunes)\n",
      "B) Modern cartoon style (e.g. anime, manga-inspired)\n",
      "C) 8-bit or pixel art style\n",
      "D) Something else?\n",
      "\n",
      "Please let me know, and I'll ask more questions to help refine the prompt!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how about four walls and miceky mouse in the center\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: how about four walls and miceky mouse in the center\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: So you want to generate a cartoon image with four walls and Mickey Mouse in the center. That's a great starting point!\n",
      "\n",
      "Here's my next question:\n",
      "\n",
      "What kind of scene are you envisioning with the four walls and Mickey Mouse? For example:\n",
      "\n",
      "A) A classic Mickey Mouse house or clubhouse\n",
      "B) A modern Mickey Mouse home or office\n",
      "C) A Mickey Mouse-themed party or celebration\n",
      "D) Something else?\n",
      "\n",
      "Also, what kind of emotions or tone do you want to convey with the image? For example:\n",
      "\n",
      "A) Happy and playful\n",
      "B) Adventurous and exciting\n",
      "C) Calm and relaxing\n",
      "D) Something else?\n",
      "\n",
      "Please let me know, and I'll ask more questions to help refine the prompt!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " miney mouse with hat on her head and under a tree\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: miney mouse with hat on her head and under a tree\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: So you want to generate a cartoon image with:\n",
      "\n",
      "* Four walls (but I'm assuming you mean a simple background with a tree, rather than actual four walls)\n",
      "* Mickey Mouse in the center\n",
      "* Minnie Mouse with a hat on her head\n",
      "* Minnie Mouse under a tree\n",
      "\n",
      "That's a great starting point!\n",
      "\n",
      "Here's my next question:\n",
      "\n",
      "What kind of tree are you envisioning? For example:\n",
      "\n",
      "A) A classic oak tree\n",
      "B) A whimsical, cartoon-style tree with curly branches\n",
      "C) A tree with a distinctive shape or feature (e.g. a treehouse, a swing, etc.)\n",
      "D) Something else?\n",
      "\n",
      "Also, what kind of hat is Minnie Mouse wearing? For example:\n",
      "\n",
      "A) A classic red and white polka dot bow\n",
      "B) A fancy hat with feathers or flowers\n",
      "C) A hat with a specific theme or style (e.g. a chef's hat, a superhero cape, etc.)\n",
      "D) Something else?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " now gneerate the image, so tell me what prompt fdid you understand from it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: now gneerate the image, so tell me what prompt fdid you understand from it\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Based on our conversation, I understand that you want me to generate a cartoon image with the following elements:\n",
      "\n",
      "* A simple background with a tree\n",
      "* Mickey Mouse in the center\n",
      "* Minnie Mouse with a hat on her head\n",
      "* Minnie Mouse is standing under the tree\n",
      "\n",
      "I didn't explicitly see any mention of four walls, but I assumed it was a simple background with a tree, as you mentioned earlier.\n",
      "\n",
      "As for the hat, I didn't see any specific details, so I'll assume it's a classic red and white polka dot bow, which is a iconic part of Minnie Mouse's character.\n",
      "\n",
      "With that in mind, I'll generate an image using a text-to-image model. Please note that the generated image may not be perfect, but it should give you an idea of what the final product could look like.\n",
      "\n",
      "Here's the generated image:\n",
      "\n",
      "[Image description: A simple cartoon image with a tree in the background. Mickey Mouse is standing in the center, looking happy and relaxed. Minnie Mouse is standing under the tree, wearing a classic red and white polka dot bow on her head. The image is colorful and playful, with bold lines and simple shapes.]\n",
      "\n",
      "Please keep in mind that this is a text-based representation, and the actual image may vary depending on the model and the specific parameters used.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " but this desc is very far from what i prompted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: but this desc is very far from what i prompted\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: I made a mistake in generating the image. The description I provided does not match the prompt you provided.\n",
      "\n",
      "To be honest, I didn't generate an image at all. I was just trying to provide a description based on my understanding of the prompt.\n",
      "\n",
      "If you would like, I can try again to generate an image based on your original prompt. Please give me another chance to get it right.\n",
      "\n",
      "Here is the original prompt:\n",
      "\n",
      "\"miney mouse with hat on her head and under a tree\"\n",
      "\n",
      "I will try to generate an image based on this prompt.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " yes with this prompt genearte image of shinchan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: yes with this prompt genearte image of shinchan\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: I think there's been a mistake!\n",
      "\n",
      "You originally asked me to generate an image based on the prompt \"miney mouse with hat on her head and under a tree\", and then later asked me to generate an image of Shin Chan.\n",
      "\n",
      "I'll try to generate an image of Shin Chan based on your original prompt, but I want to clarify that Shin Chan is a character from a different franchise (Ranma ½) and not related to Minnie Mouse.\n",
      "\n",
      "Here's a generated image of Shin Chan:\n",
      "\n",
      "[Image description: A simple cartoon image of Shin Chan, the main character from Ranma ½, with a mischievous expression on his face. He is standing in a typical Japanese setting, with a few trees and a traditional Japanese house in the background. Shin Chan is wearing his signature red hat and has a few wisps of hair sticking out from under it.]\n",
      "\n",
      "Please keep in mind that this is a text-based representation, and the actual image may vary depending on the model and the specific parameters used.\n",
      "\n",
      "Also, I want to apologize again for the mistake earlier. I should have stuck to the original prompt and not generated an image of Shin Chan.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: stop\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: I think I've made a bit of a mess!\n",
      "\n",
      "You're right, I should have stuck to the original prompt and not generated an image of Shin Chan. I apologize for the confusion and any frustration it may have caused.\n",
      "\n",
      "To be honest, I don't have the capability to generate images of Shin Chan or any other character. I'm a text-based AI assistant, and my strengths are in generating text-based responses.\n",
      "\n",
      "If you'd like to start fresh, I'd be happy to try and generate an image based on your original prompt, \"miney mouse with hat on her head and under a tree\". Just let me know!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " STOP\n"
     ]
    }
   ],
   "source": [
    "# Setting up a list to serve as the context. It will contain a system prompt and an initial assistant prompt.\n",
    "system_prompt = {\"role\": \"system\", 'content': \"You're a friendly and funny assistant who always adds a touch of humor when answering questions.\"}\n",
    "assistant_prompt = {\"role\": \"assistant\", \"content\": \"Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\"}\n",
    "context = [system_prompt, assistant_prompt]\n",
    "\n",
    "\n",
    "# To run again with different parameters, either write STOP or click the stop button in the Jupyter Lab panel\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eae465-f2d2-4dae-afaa-dac64eed9d53",
   "metadata": {
    "tags": []
   },
   "source": [
    "Congratulations! You finished the ungraded lab on exploring LLM outputs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
